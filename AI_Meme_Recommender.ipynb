{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPr6g1fF0AjmMqsltMO5DUg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatehirfan1/ai-meme-recommender/blob/main/AI_Meme_Recommender.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-T7g1HJILhP",
        "outputId": "4573f7dc-8ada-4782-e5b2-abcda6ad94d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision pillow git+https://github.com/openai/CLIP.git gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf_i52RwKWiU",
        "outputId": "359ea0fb-bcde-4b5d-c60e-099d3f1e391d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torch, clip, numpy as np, gradio as gr\n",
        "from PIL import Image\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "MEME_PATH = \"/content/drive/MyDrive/meme_project/memes\""
      ],
      "metadata": {
        "id": "ekVk7gO_6sbo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading CLIP Model**"
      ],
      "metadata": {
        "id": "oxHmNMUS8jQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Clip Model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "print(\"CLIP loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zcCjb7f61EW",
        "outputId": "adab9479-188c-4954-8748-da98ed2dbbc9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 317MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Encode**"
      ],
      "metadata": {
        "id": "QhtbZOCB8bmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and Encode Images\n",
        "print(\"Encoding meme images...\")\n",
        "image_files = [f for f in os.listdir(MEME_PATH) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
        "\n",
        "image_embs = []\n",
        "for img_name in image_files:\n",
        "    path = os.path.join(MEME_PATH, img_name)\n",
        "    img = preprocess(Image.open(path).convert(\"RGB\")).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        emb = clip_model.encode_image(img).cpu().numpy()\n",
        "    image_embs.append(emb)\n",
        "\n",
        "image_embs = np.vstack(image_embs)\n",
        "print(f\"Loaded {len(image_files)} memes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn9ZHa4262TF",
        "outputId": "ad3f7ea7-9b1a-4725-c966-066fbfc92721"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding meme images...\n",
            "Loaded 100 memes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recommendation Function:**"
      ],
      "metadata": {
        "id": "04Qw9evk8Mgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Meme recommendation function\n",
        "def recommend_meme(user_text):\n",
        "    txt_emb = clip_model.encode_text(clip.tokenize(user_text).to(device)).cpu().detach().numpy()\n",
        "    sims = cosine_similarity(txt_emb, image_embs)[0]\n",
        "    best_idx = np.argmax(sims)\n",
        "    best_meme = image_files[best_idx]\n",
        "    return Image.open(os.path.join(MEME_PATH, best_meme))"
      ],
      "metadata": {
        "id": "JCMgKeCU64Xt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UI:**\n"
      ],
      "metadata": {
        "id": "Fu8Vb73T77wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UI\n",
        "ui = gr.Interface(\n",
        "    fn=recommend_meme,\n",
        "    inputs=gr.Textbox(label=\"Enter your situation\", placeholder=\"e.g. When you study all night and still fail...\"),\n",
        "    outputs=gr.Image(label=\"Best Matching Meme\"),\n",
        "    title=\"AI Meme Recommender \",\n",
        "    description=\"Type your situation and AI will find the best meme for it using CLIP.\"\n",
        ")\n",
        "\n",
        "ui.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "X033U_Aj669_",
        "outputId": "20f3b7db-ba1c-4a7d-acb2-632ad79ce9da"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b24e63af5d16ade02d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b24e63af5d16ade02d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}